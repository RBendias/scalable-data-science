{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook:\n",
    "* Loading spatial data with geospark\n",
    "* Using JoinQuery without indexing\n",
    "* Trying different indexing methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:04:53.763026Z",
     "start_time": "2021-02-11T13:04:53.722573Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:04:53.774980Z",
     "start_time": "2021-02-11T13:04:53.767672Z"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:04:55.644002Z",
     "start_time": "2021-02-11T13:04:53.783720Z"
    }
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from time import time\n",
    "import geopandas as gpd\n",
    "import copy\n",
    "from pyspark.sql import SparkSession\n",
    "from geospark.core.SpatialRDD import PointRDD\n",
    "from geospark.core.SpatialRDD import PolygonRDD\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import SQLContext\n",
    "from geospark.register import GeoSparkRegistrator\n",
    "from geospark.utils import GeoSparkKryoRegistrator, KryoSerializer\n",
    "from geospark.register import upload_jars\n",
    "from geospark.core.enums import FileDataSplitter\n",
    "from geospark.core.spatialOperator import RangeQuery\n",
    "from geospark.sql.types import GeometryType\n",
    "from geospark.core.enums import IndexType\n",
    "from geospark.core.spatialOperator import JoinQuery\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from geospark.core.enums import GridType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:04:55.658488Z",
     "start_time": "2021-02-11T13:04:55.648488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SPARK_HOME=/opt/conda/envs/geospark_demo/lib/python3.7/site-packages/pyspark\n",
      "env: ARROW_PRE_0_15_IPC_FORMAT=1\n",
      "env: JAVA_HOME=/opt/conda/envs/geospark_demo\n"
     ]
    }
   ],
   "source": [
    "%env SPARK_HOME /opt/conda/envs/geospark_demo/lib/python3.7/site-packages/pyspark\n",
    "%env ARROW_PRE_0_15_IPC_FORMAT 1\n",
    "%env JAVA_HOME /opt/conda/envs/geospark_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:05:02.667091Z",
     "start_time": "2021-02-11T13:04:55.661586Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generate spark session\n",
    "upload_jars()\n",
    "spark = SparkSession.builder.\\\n",
    "        master(\"local[1]\").\\\n",
    "        appName(\"SpatialAnalysis\").\\\n",
    "        config(\"spark.serializer\", KryoSerializer.getName).\\\n",
    "        config(\"spark.kryo.registrator\", GeoSparkKryoRegistrator.getName) .\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:05:08.553706Z",
     "start_time": "2021-02-11T13:05:02.670545Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GeoSparkRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:05:08.579985Z",
     "start_time": "2021-02-11T13:05:08.558711Z"
    }
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:05:08.601719Z",
     "start_time": "2021-02-11T13:05:08.582125Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"../../../data/HW2/nyc-data\"\n",
    "nyc_tweets_location = \"file://\" + os.path.abspath(data_path)+ \"/nyc-tweets.txt\"\n",
    "nyc_neighborhoods_location = \"file://\" + os.path.abspath(data_path)+ \"/nyc-neighborhoods.wkt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:05:08.612775Z",
     "start_time": "2021-02-11T13:05:08.606911Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading point data set from twitter tweets \n",
    "def load_data():\n",
    "    point_rdd = PointRDD(sc, nyc_tweets_location, 0, FileDataSplitter.CSV, False) #, 10, StorageLevel.MEMORY_ONLY, \"epsg:4326\", \"epsg:4326\")\n",
    "    point_rdd.analyze()\n",
    "    point_rdd.spatialPartitioning(GridType.KDBTREE)\n",
    "\n",
    "    # Loading polygon dataset corresponding to the neighborhood regions in New York\n",
    "    polygon_rdd = PolygonRDD(sc, nyc_neighborhoods_location, FileDataSplitter.WKT, False)\n",
    "    polygon_rdd.spatialPartitioning(point_rdd.getPartitioner())\n",
    "    return point_rdd, polygon_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:16:18.211574Z",
     "start_time": "2021-02-11T17:13:43.224842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 175.24 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit point_rdd, polygon_rdd = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the numbers of tweets that are contained within each one of the neighborhoord polygons without using spatial indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T17:16:18.241428Z",
     "start_time": "2021-02-11T17:16:18.215435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 0.021333694458007812 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "spatial_join_result_non_flat = JoinQuery.SpatialJoinQuery(point_rdd, polygon_rdd, False, False)\n",
    "print(f\"Finished after {time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:35:34.841656Z",
     "start_time": "2021-02-11T13:07:23.263806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|        neighborhood|number_of_points|\n",
      "+--------------------+----------------+\n",
      "|POLYGON ((-73.740...|           53103|\n",
      "|POLYGON ((-73.883...|          175269|\n",
      "|POLYGON ((-73.975...|          318134|\n",
      "|POLYGON ((-73.847...|          110461|\n",
      "|POLYGON ((-73.902...|          299634|\n",
      "|POLYGON ((-73.898...|          128726|\n",
      "|POLYGON ((-73.918...|          101676|\n",
      "|POLYGON ((-73.932...|          283657|\n",
      "|POLYGON ((-73.883...|          140596|\n",
      "|POLYGON ((-73.793...|           85819|\n",
      "|POLYGON ((-73.887...|          172802|\n",
      "|POLYGON ((-73.862...|           71699|\n",
      "|POLYGON ((-73.894...|           62533|\n",
      "|POLYGON ((-73.974...|          111565|\n",
      "|POLYGON ((-73.993...|          499036|\n",
      "|POLYGON ((-73.854...|           69036|\n",
      "|POLYGON ((-73.992...|          123127|\n",
      "|POLYGON ((-73.940...|          184815|\n",
      "|POLYGON ((-73.896...|           85414|\n",
      "|POLYGON ((-73.849...|          117237|\n",
      "+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Created dataframe in 1691.569635629654 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "number_of_points = spatial_join_result_non_flat.map(lambda x: [x[0].geom, x[1].__len__()])\n",
    "schema = StructType([\n",
    "    StructField(\"neighborhood\", GeometryType(), False),\n",
    "    StructField(\"number_of_points\", LongType(), False)\n",
    "])\n",
    "df = spark.createDataFrame(number_of_points, schema, verifySchema=False)\n",
    "df.show()\n",
    "print(f\"Created dataframe in {time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:44:49.873211Z",
     "start_time": "2021-02-11T13:35:34.844876Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "df.coalesce(1).toPandas().to_csv('output.csv',  index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the numbers of tweets that are contained within each one of the neighborhoord polygons using spatial indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T13:44:49.878866Z",
     "start_time": "2021-02-11T13:44:49.875190Z"
    }
   },
   "outputs": [],
   "source": [
    "index_types = [False, IndexType.QUADTREE, IndexType.RTREE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T14:07:08.172225Z",
     "start_time": "2021-02-11T13:44:49.882072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Point index: False, Polygon index: False\n",
      "Finished after 0.027492046356201172 seconds \n",
      "\n",
      " Point index: False, Polygon index: IndexType.QUADTREE\n",
      "Finished after 0.019048213958740234 seconds \n",
      "\n",
      " Point index: False, Polygon index: IndexType.RTREE\n",
      "Finished after 0.015477657318115234 seconds \n",
      "\n",
      " Point index: IndexType.QUADTREE, Polygon index: False\n",
      "Finished after 0.018654346466064453 seconds \n",
      "\n",
      " Point index: IndexType.QUADTREE, Polygon index: IndexType.QUADTREE\n",
      "Finished after 0.013472318649291992 seconds \n",
      "\n",
      " Point index: IndexType.QUADTREE, Polygon index: IndexType.RTREE\n",
      "Finished after 0.01818108558654785 seconds \n",
      "\n",
      " Point index: IndexType.RTREE, Polygon index: False\n",
      "Finished after 0.014382600784301758 seconds \n",
      "\n",
      " Point index: IndexType.RTREE, Polygon index: IndexType.QUADTREE\n",
      "Finished after 0.02059030532836914 seconds \n",
      "\n",
      " Point index: IndexType.RTREE, Polygon index: IndexType.RTREE\n",
      "Finished after 0.013378143310546875 seconds \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for point_index in index_types:\n",
    "    for polygon_index in index_types:\n",
    "        point_rdd, polygon_rdd = load_data()\n",
    "        if point_index:\n",
    "            point_rdd.buildIndex(point_index, True)\n",
    "        if polygon_index:\n",
    "            polygon_rdd.buildIndex(polygon_index, True)\n",
    "\n",
    "        start = time()\n",
    "        spatial_join_result_non_flat = JoinQuery.SpatialJoinQuery(point_rdd, polygon_rdd, True, False)\n",
    "        print(f\" Point index: {point_index}, Polygon index: {polygon_index}\")\n",
    "        print(f\"Finished after {time() - start} seconds \\n\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "geospark_demo (conda, /opt/conda/envs/geospark_demo)",
   "language": "python",
   "name": "conda_test_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
